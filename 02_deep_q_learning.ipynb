{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "75d2496b-6956-4f27-bf9d-82a3064130f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "8abc4b7e-fa0d-464c-99f2-3e516846f69c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(2)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "env.action_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd158209-383d-4cac-92cc-4f353124dc03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(0),\n",
       " np.int64(1),\n",
       " np.int64(1),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0),\n",
       " np.int64(0)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[env.action_space.sample() for _ in range(10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "28c0d9a5-0ea2-4f48-a1f9-6c664e75bcd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Box([-4.8               -inf -0.41887903        -inf], [4.8               inf 0.41887903        inf], (4,), float32)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0a3e074b-a223-4173-a103-ff1858549530",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4,)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0d751b3a-3731-443e-80d6-9a43faa52b8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03349816,  0.0096554 , -0.02111368, -0.04570484], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset(seed=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bfa10d42-1fc6-4277-bff7-a9305d16e3f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03369127, -0.18515752, -0.02202777,  0.24024247], dtype=float32),\n",
       " 1.0,\n",
       " False,\n",
       " False,\n",
       " {})"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.step(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a4d7c2-190c-48fa-b01f-5a7cd0a6d4ba",
   "metadata": {},
   "source": [
    "## A Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "9888cbb7-d0c3-451d-a61a-47639db16b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "    def play(self, episodes=1):\n",
    "        self.total_rewards = []\n",
    "        for e in range(episodes):\n",
    "            self.env.reset()\n",
    "\n",
    "            for step in range(1, 100):\n",
    "                action = self.env.action_space.sample()\n",
    "                state, reward, done, trunc, info = self.env.step(action)\n",
    "                if done:\n",
    "                    self.total_rewards.append(step)\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "dd249e86-2d5c-4eaa-ac24-1536cd9739e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.6"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ra = RandomAgent()\n",
    "ra.play(15)\n",
    "average_reward = round(sum(ra.total_rewards) / len(ra.total_rewards), 2)\n",
    "average_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ec9bd125-add0-43d8-8588-a9d195c33265",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import deque\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "96a77afe-7369-4ee1-80a2-3ed36777a587",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps device\n"
     ]
    }
   ],
   "source": [
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "38e0925c-4f31-49a9-a3d9-42237e127da2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNetwork(\n",
       "  (linear_relu_stack): Sequential(\n",
       "    (0): Linear(in_features=4, out_features=24, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=24, out_features=24, bias=True)\n",
       "    (3): ReLU()\n",
       "    (4): Linear(in_features=24, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(4, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 24),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(24, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "e5c74534-5232-4210-b52d-1f15a481040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQLAgent:\n",
    "    def __init__(self):\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.9975\n",
    "        self.epsilon_min = 0.1\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.batch_size = 12\n",
    "        self.gamma = 0.9\n",
    "        self.max_reward = 0\n",
    "        self.model = NeuralNetwork().to(device)\n",
    "        self.env = gym.make(\"CartPole-v1\")\n",
    "        self.opt = torch.optim.Adam(self.model.parameters(), lr=1e-3)\n",
    "        self.criterion = torch.nn.MSELoss()\n",
    "\n",
    "    def act(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return self.env.action_space.sample()\n",
    "\n",
    "        return torch.argmax(self.model(state).detach(), dim=1).item()\n",
    "\n",
    "    def replay(self):\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        for state, action, next_state, reward, done in batch:\n",
    "            if not done:\n",
    "                expect = torch.max(self.model(next_state).detach(), dim=1)[0]\n",
    "                reward += self.gamma * expect\n",
    "            target = self.model(state)\n",
    "            loss = self.criterion(target[:, action], torch.Tensor([reward]).to(device))\n",
    "            loss.backward()\n",
    "            self.opt.step()\n",
    "            self.opt.zero_grad()\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "    def learn(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            observation, _ = self.env.reset()\n",
    "            state = torch.Tensor(observation).view(-1, 4).to(device)\n",
    "            for f in range(1, 5000):\n",
    "                action = self.act(state)\n",
    "                observation, reward, done, trunc, _info = self.env.step(action)\n",
    "                next_state = torch.Tensor(observation).view(-1, 4).to(device)\n",
    "                self.memory.append([state, action, next_state, reward, done])\n",
    "                state = next_state\n",
    "\n",
    "                if done or trunc:\n",
    "                    self.max_reward = max(self.max_reward, f)\n",
    "                    templ = f\"episode={e:4d} | total_reward={f:4d} | max={self.max_reward:4d}\"\n",
    "                    if e % 100 == 0:\n",
    "                        print(templ, end=\"\\r\")\n",
    "                    break\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                self.replay()\n",
    "\n",
    "    def test(self, episodes):\n",
    "        for e in range(1, episodes + 1):\n",
    "            observation, _ = self.env.reset()\n",
    "            for f in range(1, 5000):\n",
    "                state = torch.Tensor(observation).view(-1, 4).to(device)\n",
    "                with torch.no_grad():\n",
    "                    logits = self.model(state)\n",
    "                action = torch.argmax(logits, dim=1)[0].item()\n",
    "                observation, reward, done, trunc, _info = self.env.step(action)\n",
    "                if done or trunc:\n",
    "                    print(f, end=\" \")\n",
    "                    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "ec38f396-541d-4b3c-b0e1-a6e16cd025c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode=2000 | total_reward= 150 | max= 500"
     ]
    }
   ],
   "source": [
    "agent = DQLAgent()\n",
    "agent.learn(2000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2e8ab128-3147-4a7d-b4e5-b6d67e8b78ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 97 202 336 118 178 190 142 186 160 CPU times: user 319 ms, sys: 196 ms, total: 515 ms\n",
      "Wall time: 700 ms\n"
     ]
    }
   ],
   "source": [
    "%time agent.test(10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
